"""Wrapper optimizer for Elastic Average SGD """import tensorflow as tffrom tensorflow.python.training import optimizerfrom tensorflow.python.ops import resource_variable_opsLOCAL_VARIABLE_NAME = 'glocal_variable'GLOBAL_VARIABLE_NAME = 'global_variable'RECORD_AVERAGE_VARIABLE_NAME = 'record_local_variable'class FlockingCustomGetter(object):    """custom_getter is used with tf.get_variable instead of tf.Variablel.    This custom_getter is used to:    1. place the trainable variables(local_variables) to worker_device(gpu)    2. generate global_variables(a copy of_local variables) and place them on cpu,     which are untrainable are can be assessed by other worker to calculate the flocking_magnitude    3. generate record_local_variables which are untrainable and are placed on worker_device(gpu).     The number of record_local_variables is num_workers * local_variables. They are     used to record the global_variables of all workers(including the current worker).    For example,    flocking_custom_getter = FlockingCustomGetter(num_workers, worker_device)    with tf.variable_scope("worker_1", custom_getter=flocking_custom_getter):        weights = tf.get_variable(initializer=tf.truncated_normal([28*28, 30], stddev=1.0/28), name="weights")        biases = tf.get_variable(initializer=tf.zeros([30]), name="biases")    """    def __init__(self, flocking_workers, worker_index):        """create a new 'FlockingCustomGetter'."""        self.worker_index = worker_index        self.flocking_workers = flocking_workers        self._index = 10    def __call__(self, getter, name, trainable, collections, *args, **kwargs):        if trainable:            with tf.device('/job:worker/task:%d/gpu:%d' % (self.worker_index, self.worker_index)):                local_variables = getter(                    name=name,                    trainable=True,                    collections=[tf.GraphKeys.LOCAL_VARIABLES, '%s_worker_%d' % (LOCAL_VARIABLE_NAME, self.worker_index)],                    *args,                    **kwargs)                # record_average_variables = tf.Variable(name='record_%s' % name,                #                                        initial_value=local_variables.initialized_value,                #                                        trainable=False,                #                                        collections=[tf.GraphKeys.LOCAL_VARIABLES,                #                                                     '%s_worker_%d'                #                                                     % (RECORD_AVERAGE_VARIABLE_NAME, self.worker_index)])            # for index in self.flocking_workers:            #     global_variables = tf.Variable(name='worker_%d_%d' % (index, self._index),            #                                    initial_value=local_variables.initialized_value,            #                                    trainable=False,            #                                    collections=[tf.GraphKeys.GLOBAL_VARIABLES,            #                                                 '%s_worker_%d' % (GLOBAL_VARIABLE_NAME, index)])            # self._index += 1            return local_variables        else:            return getter(name, trainable, collections, *args, **kwargs)class FlockingOptimizer(optimizer.Optimizer):    """Wrapper optimizer that implements the Flocking SGD algorithm. This is an async    optimizer. During the training, each worker will copy all global_variables and stores    them at record_local_variables. And then each worker will calculate the average of    record_local_variables based on the 0 dimension which is actually the center for    each variable from different worker. Each worker then calcualte the distance between    local_variables and the average of record_local_variables. The distance is used to    calculate the flocking_magnitude to update both the local_variables and global_variables.    Also, after each worker updates the local_variables, the worker's own local_step will    be incremented by 1; after corresponding global_variables are updated, global_step will    be incremented by 1    """    def __init__(self,                 opt,                 flocking_workers,                 attraction=0.0,                 repulsion=0.0,                 dis=0.001,                 use_locking=False,                 name='FlockingOptimizer'):        super(FlockingOptimizer, self).__init__(use_locking, name)        self._opt = opt        self._attra = attraction        self._repul = repulsion        self._dis = dis        self._slope = self._repul / self._dis        self._worker_index = flocking_workers[-1]        self._flocking_workers_except_current_worker = flocking_workers[:-1]        with tf.variable_scope('local_step_%d' % self._worker_index):            self._local_step = tf.get_variable(initializer=0,                                               trainable=False,                                               collections=[tf.GraphKeys.LOCAL_VARIABLES],                                               name='')        self._opt._prepare()    def compute_gradients(self,                          loss,                          gate_gradients=optimizer.Optimizer.GATE_OP,                          aggregation_method=None,                          colocate_gradients_with_ops=False,                          grad_loss=None):        return self._opt.compute_gradients(loss,                                           tf.get_collection('%s_worker_%d' % (GLOBAL_VARIABLE_NAME, self._worker_index)),                                           gate_gradients,                                           aggregation_method,                                           colocate_gradients_with_ops,                                           grad_loss)    def apply_gradients_and_flocking(self, grads_and_vars, global_step=None, name=None):        """apply flocking_magnitude and gradients for local_variables of current worker"""        current_worker_vars = tf.get_collection('%s_worker_%d' % (GLOBAL_VARIABLE_NAME, self._worker_index))        def _apply_flocking():            with tf.device('/job:ps/task:0'):                print('this is flocking worker exce', self._flocking_workers_except_current_worker)                same_variables_from_different_workers = list(zip(                    *(tf.get_collection('%s_worker_%d' % (GLOBAL_VARIABLE_NAME, index))                      for index in self._flocking_workers_except_current_worker)))                print(same_variables_from_different_workers)                same_variables_ops = tf.Print(same_variables_from_different_workers[0][0],                                              [same_variables_from_different_workers[0][0].read_value()],                                              'this is worker %d, variable in worker_0 ' % self._worker_index)                average_variables = [tf.add_n([i for i in s_v]) / len(self._flocking_workers_except_current_worker)                                     for s_v in same_variables_from_different_workers]                av = tf.Print(average_variables[0], [average_variables[0]],                              "worker_%d, average variables is:" % self._worker_index)                distance = [tf.subtract(var.read_value(), average) for var, average in                            zip(current_worker_vars, average_variables)]                di = tf.Print(distance[0], [distance[0]], 'worker_%d, Distance is: ' % self._worker_index)                flocking_function = [                    tf.minimum(self._slope * tf.abs(dis) - self._repul + self._attra, self._attra)                    for dis in distance]                ff = tf.Print(flocking_function[0][0], [flocking_function[0][0]],                              'worker_%d, flocking_function is: ' % self._worker_index)                flocking_magnitudes = [tf.multiply(dis, f) for dis, f in                                       zip(distance, flocking_function)]                fm = tf.Print(flocking_magnitudes[0][0], [flocking_magnitudes[0][0]],                              'worker_%d, flocking_magnitudes is: ' % self._worker_index)                current_worker_vars_update = [tf.assign(var, var.read_value() - f_mag) for var, f_mag in                                zip(current_worker_vars, flocking_magnitudes)]                current_worker_vars_update.append(same_variables_ops)                # flocking_ops.append(av)                # flocking_ops.append(di)                current_worker_vars_update.append(ff)                # current_worker_vars_update.append(fm)                # with tf.control_dependencies(flocking_ops):                #     global_var_update_ops = []                #     global_var_update_ops = [tf.assign(g_v, l_v.read_value()) for g_v, l_v in                #                              zip(tf.get_collection('%s_worker_%d'                #                                                    % (GLOBAL_VARIABLE_NAME, self._worker_index)),                #                                  local_var_list)]                #                #     if False:  # global_step:                #         with tf.colocate_with(global_step):                #             global_var_update_ops.append(tf.assign_add(global_step, 1))                #     v_up = tf.Print(local_var_list[0], [local_var_list[0]],                #                     "worker_%d, local_variables after update is: " % self._worker_index)                #     # global_var_update_ops.append(v_up)                #     with tf.control_dependencies(global_var_update_ops):                #         global_op = tf.Print(                #             tf.get_collection('%s_worker_%d' % (GLOBAL_VARIABLE_NAME, self._worker_index))[0],                #             [tf.get_collection('%s_worker_%d' % (GLOBAL_VARIABLE_NAME, self._worker_index))[                #                  0].read_value()],                #             'this is worker %d, global variable after update is:' % self._worker_index)                #         # global_var_update_ops.append(global_op)                #                group_op = tf.group(current_worker_vars_update)            return group_op        ##apply gradients first        apply_gradients_ops = self._opt.apply_gradients(grads_and_vars)        with tf.control_dependencies([apply_gradients_ops]):            update = _apply_flocking()            print('here\nhere\n')        return update