"""Wrapper optimizer for Elastic Average SGD """import tensorflow as tffrom tensorflow.python.training import optimizerSHARED_VARIABLES_COLLECTION = 'shared_variables'class FlockingOptimizer(optimizer.Optimizer):    """Wrapper optimizer that implements the Flocking SGD algorithm. This is an async    optimizer. During the training, each tower will copy all global_variables and stores    them at record_local_variables. And then each tower will calculate the average of    record_local_variables based on the 0 dimension which is actually the center for    each variable from different tower. Each tower then calcualte the distance between    local_variables and the average of record_local_variables. The distance is used to    calculate the flocking_magnitude to update both the local_variables and global_variables.    Also, after each tower updates the local_variables, the tower's own local_step will    be incremented by 1; after corresponding global_variables are updated, global_step will    be incremented by 1    """    def __init__(self,                 opt,                 attraction=0.0,                 repulsion=0.0,                 dis=0.001,                 use_locking=False,                 name='FlockingOptimizer'):        super(FlockingOptimizer, self).__init__(use_locking, name)        self._opt = opt        self._attraction = attraction        self._repulsion = repulsion        self._dis = dis        self._slope = self._repulsion / self._dis        self._opt._prepare()    def minimize_with_flocking(self, loss,                               flocking_workers,                               gate_gradients=optimizer.Optimizer.GATE_OP,                               aggregation_method=None,                               colocate_gradients_with_ops=False,                               grad_loss=None,                               global_step=None):        print(flocking_workers)        current_worker_index = flocking_workers[-1]        flocking_workers_except_current_worker = flocking_workers[:-1]        num_flocking_workers_except_current_worker = len(flocking_workers_except_current_worker)        current_tower_vars = tf.get_collection_ref('%s_tower_%d' % (SHARED_VARIABLES_COLLECTION, current_worker_index))        worker_device = '/job:worker/task:%d' % current_worker_index        ps_device = '/job:ps/task:0'        with tf.device(worker_device):            grads_and_vars = self._opt.compute_gradients(loss,                                                         gate_gradients=gate_gradients,                                                         aggregation_method=aggregation_method,                                                         colocate_gradients_with_ops=colocate_gradients_with_ops,                                                         grad_loss=grad_loss)        var_before_gradient = tf.Print(current_tower_vars[0], [current_tower_vars[0]],                                       'var before gradient is')        with tf.control_dependencies([]):            with tf.device(ps_device):                apply_gradients_ops = self._opt.apply_gradients(grads_and_vars)        def _apply_flocking():            with tf.device(ps_device):                # a list to store shared variables from flocking_workers                flocking_variables = (tf.get_collection_ref('%s_tower_%d' % (SHARED_VARIABLES_COLLECTION, i))                                      for i in flocking_workers_except_current_worker)                same_variables_from_different_towers = zip(*flocking_variables)                # same_variables_from_different_towers = [i for i in zip(*flocking_variables)]                # rv = [tf.Print(same_variables_from_different_towers[0][i],                #                [same_variables_from_different_towers[0][i].read_value()],                #                "this is in worker %d, record local for worker %d is: " % (current_worker_index, tower))                #       for i, tower in enumerate(flocking_workers_except_current_worker)]                average_variables = [                    tf.add_n([i.read_value() for i in s_v]) / num_flocking_workers_except_current_worker                    for s_v in same_variables_from_different_towers]                av = tf.Print(average_variables[0], [average_variables[0]],                              "tower_%d, average variables is:" % current_worker_index, )                gp = tf.Print(grads_and_vars[0][0], [grads_and_vars[0][0]],                              'tower_%d, gradients is: ' % current_worker_index, )                            with tf.device(worker_device):                distance = [tf.subtract(local.read_value(), average) for local, average in                            zip(current_tower_vars, average_variables)]                di = tf.Print(distance[0], [distance[0]], 'tower_%d, Distance is: ' % current_worker_index, )                flocking_function = [                    tf.minimum(self._slope * tf.abs(dis) - self._repulsion + self._attraction, self._attraction)                    for dis in distance]                ff = tf.Print(flocking_function[0][0], [flocking_function[0][0]],                              'tower_%d, flocking_function is: ' % current_worker_index, )                flocking_magnitudes = [tf.multiply(dis, f) for dis, f in                                       zip(distance, flocking_function)]                fm = tf.Print(flocking_magnitudes[0][0], [flocking_magnitudes[0][0]],                              'tower_%d, flocking_magnitudes is: ' % current_worker_index, )                current_tower_vars_update = [tf.assign(var, var.read_value() - f_mag) for var, f_mag in                                         zip(current_tower_vars, flocking_magnitudes)]                # if current_worker_index == 100:                #     current_tower_vars_update.extend(rv)                #     # current_tower_vars_update.append(rv1)                #                #     current_tower_vars_update.append(av)                #                #     current_tower_vars_update.append(di)                #     current_tower_vars_update.append(gp)                #     current_tower_vars_update.append(ff)                #     current_tower_vars_update.append(fm)                # with tf.control_dependencies(current_tower_vars_update):                #     var_after_flocking = tf.Print(current_tower_vars[0], [current_tower_vars[0]],                #                                   'var after flocking is')                #     # current_tower_vars_update.append(var_after_flocking)                #     if False:                #         with tf.colocate_with(global_step):                #             current_tower_vars_update.append(tf.assign_add(global_step, 1))            group_op = tf.group(current_tower_vars_update)            return group_op        with tf.control_dependencies([apply_gradients_ops]):            update = _apply_flocking()        return update